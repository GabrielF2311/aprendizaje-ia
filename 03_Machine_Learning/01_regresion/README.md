# ğŸ§® Machine Learning BÃ¡sico - RegresiÃ³n Lineal

## ğŸ¯ Objetivos de Aprendizaje

Al finalizar esta semana, podrÃ¡s:
- Entender quÃ© es la regresiÃ³n lineal y cuÃ¡ndo usarla
- Implementar regresiÃ³n lineal desde cero
- Usar scikit-learn para regresiÃ³n
- Evaluar modelos con mÃ©tricas apropiadas
- Visualizar resultados y diagnÃ³sticos

## ğŸ“š Contenido

### 1. TeorÃ­a Fundamental

#### Â¿QuÃ© es RegresiÃ³n Lineal?

La **regresiÃ³n lineal** es un algoritmo para predecir valores continuos. Encuentra la mejor lÃ­nea que se ajusta a los datos.

**Ejemplo**: Predecir precio de casas basado en tamaÃ±o
- **Input (X)**: Metros cuadrados
- **Output (y)**: Precio
- **Modelo**: Encuentra una lÃ­nea que relacione X con y

#### EcuaciÃ³n

**RegresiÃ³n lineal simple** (una variable):
$$y = mx + b$$

Donde:
- `y`: Variable a predecir (target)
- `x`: Variable independiente (feature)
- `m`: Pendiente (slope, weight)
- `b`: Intercepto (bias)

**RegresiÃ³n lineal mÃºltiple** (varias variables):
$$y = w_1x_1 + w_2x_2 + ... + w_nx_n + b$$

O en forma vectorial:
$$\hat{y} = \mathbf{w}^T\mathbf{x} + b$$

#### Â¿CÃ³mo Encontrar m y b?

**Objetivo**: Minimizar el error entre predicciones y valores reales.

**FunciÃ³n de costo (MSE - Mean Squared Error)**:
$$J(w, b) = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$

**MÃ©todos para optimizar**:
1. **EcuaciÃ³n Normal** (soluciÃ³n cerrada)
2. **Descenso por Gradiente** (iterativo)

### 2. ImplementaciÃ³n desde Cero

```python
import numpy as np

class LinearRegression:
    """RegresiÃ³n lineal implementada desde cero"""
    
    def __init__(self):
        self.weights = None
        self.bias = None
    
    def fit(self, X, y, learning_rate=0.01, epochs=1000):
        """
        Entrena el modelo usando descenso por gradiente.
        
        Args:
            X: Features (n_samples, n_features)
            y: Target (n_samples,)
            learning_rate: Tasa de aprendizaje
            epochs: NÃºmero de iteraciones
        """
        n_samples, n_features = X.shape
        
        # Inicializar parÃ¡metros
        self.weights = np.zeros(n_features)
        self.bias = 0
        
        # Descenso por gradiente
        for epoch in range(epochs):
            # PredicciÃ³n
            y_pred = np.dot(X, self.weights) + self.bias
            
            # Calcular gradientes
            dw = (1/n_samples) * np.dot(X.T, (y_pred - y))
            db = (1/n_samples) * np.sum(y_pred - y)
            
            # Actualizar parÃ¡metros
            self.weights -= learning_rate * dw
            self.bias -= learning_rate * db
            
            # Opcional: imprimir progreso
            if epoch % 100 == 0:
                mse = np.mean((y_pred - y) ** 2)
                print(f"Epoch {epoch}, MSE: {mse:.4f}")
    
    def predict(self, X):
        """Hace predicciones"""
        return np.dot(X, self.weights) + self.bias
```

### 3. Usando scikit-learn

```python
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Datos de ejemplo
X = [[1], [2], [3], [4], [5]]
y = [2, 4, 6, 8, 10]

# Dividir datos
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Crear y entrenar modelo
model = LinearRegression()
model.fit(X_train, y_train)

# Predecir
y_pred = model.predict(X_test)

# Evaluar
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"MSE: {mse:.4f}")
print(f"RÂ²: {r2:.4f}")
print(f"Coeficientes: {model.coef_}")
print(f"Intercepto: {model.intercept_}")
```

### 4. MÃ©tricas de EvaluaciÃ³n

#### Mean Squared Error (MSE)
$$MSE = \frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y}_i)^2$$

- Penaliza mÃ¡s los errores grandes
- Siempre positivo
- Mismo unidad que yÂ²

#### Root Mean Squared Error (RMSE)
$$RMSE = \sqrt{MSE}$$

- Misma unidad que y
- MÃ¡s interpretable que MSE

#### Mean Absolute Error (MAE)
$$MAE = \frac{1}{n}\sum_{i=1}^{n}|y_i - \hat{y}_i|$$

- Menos sensible a outliers
- MÃ¡s robusto que MSE

#### RÂ² (Coeficiente de DeterminaciÃ³n)
$$R^2 = 1 - \frac{\sum(y_i - \hat{y}_i)^2}{\sum(y_i - \bar{y})^2}$$

- Rango: [0, 1] (puede ser negativo si el modelo es muy malo)
- RÂ² = 1: Ajuste perfecto
- RÂ² = 0: Modelo tan bueno como predecir la media
- Mide quÃ© % de varianza explica el modelo

### 5. VisualizaciÃ³n

```python
import matplotlib.pyplot as plt

def plot_regression(X, y, y_pred):
    """Visualiza regresiÃ³n lineal"""
    plt.figure(figsize=(10, 6))
    
    # Scatter plot de datos reales
    plt.scatter(X, y, color='blue', label='Datos reales', alpha=0.6)
    
    # LÃ­nea de regresiÃ³n
    plt.plot(X, y_pred, color='red', linewidth=2, label='PredicciÃ³n')
    
    plt.xlabel('X')
    plt.ylabel('y')
    plt.title('RegresiÃ³n Lineal')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.show()


def plot_residuals(y_true, y_pred):
    """Visualiza residuos (errores)"""
    residuals = y_true - y_pred
    
    plt.figure(figsize=(12, 5))
    
    # Subplot 1: Residuos vs predicciones
    plt.subplot(1, 2, 1)
    plt.scatter(y_pred, residuals, alpha=0.6)
    plt.axhline(y=0, color='r', linestyle='--')
    plt.xlabel('Predicciones')
    plt.ylabel('Residuos')
    plt.title('Residuos vs Predicciones')
    plt.grid(True, alpha=0.3)
    
    # Subplot 2: Histograma de residuos
    plt.subplot(1, 2, 2)
    plt.hist(residuals, bins=30, edgecolor='black', alpha=0.7)
    plt.xlabel('Residuos')
    plt.ylabel('Frecuencia')
    plt.title('DistribuciÃ³n de Residuos')
    plt.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
```

## ğŸ“ Ejercicios PrÃ¡cticos

### Ejercicio 1: ImplementaciÃ³n BÃ¡sica
Implementa regresiÃ³n lineal desde cero para predecir temperaturas.

### Ejercicio 2: MÃºltiples Features
Usa regresiÃ³n lineal mÃºltiple para predecir precios de casas con varios features.

### Ejercicio 3: ComparaciÃ³n de MÃ©tricas
Compara MSE, RMSE, MAE y RÂ² en diferentes datasets.

### Ejercicio 4: DiagnÃ³stico
Analiza residuos para detectar problemas en el modelo.

## ğŸ¯ Mini-Proyecto: Predictor de Salarios

**Objetivo**: Predecir salarios basado en aÃ±os de experiencia.

**Dataset**: `salaries.csv` (proporcionado)

**Tareas**:
1. Cargar y explorar datos
2. Visualizar relaciÃ³n entre experiencia y salario
3. Entrenar modelo de regresiÃ³n lineal
4. Evaluar con todas las mÃ©tricas
5. Visualizar lÃ­nea de regresiÃ³n
6. Analizar residuos
7. Hacer predicciones para nuevos datos

## ğŸ’¡ Tips y Buenas PrÃ¡cticas

### Feature Scaling
```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
```

### ValidaciÃ³n Cruzada
```python
from sklearn.model_selection import cross_val_score

scores = cross_val_score(model, X, y, cv=5, scoring='r2')
print(f"RÂ² promedio: {scores.mean():.4f} (+/- {scores.std():.4f})")
```

### DetecciÃ³n de Overfitting
```python
# Compara error en train vs test
train_score = model.score(X_train, y_train)
test_score = model.score(X_test, y_test)

print(f"RÂ² Train: {train_score:.4f}")
print(f"RÂ² Test: {test_score:.4f}")

if train_score - test_score > 0.1:
    print("âš ï¸ Posible overfitting")
```

## ğŸ” Cuando NO usar RegresiÃ³n Lineal

- RelaciÃ³n no lineal entre variables
- Variables categÃ³ricas (usa regresiÃ³n logÃ­stica)
- Muchos outliers (usa modelos robustos)
- Multicolinealidad alta (features muy correlacionados)

## ğŸ“š Recursos Adicionales

- StatQuest: "Linear Regression" (YouTube)
- Libro: "Introduction to Statistical Learning"
- DocumentaciÃ³n scikit-learn

## âœ… Checklist de Progreso

- [ ] Entiendo la ecuaciÃ³n de regresiÃ³n lineal
- [ ] Puedo implementar regresiÃ³n desde cero
- [ ] SÃ© usar scikit-learn para regresiÃ³n
- [ ] Conozco las mÃ©tricas de evaluaciÃ³n
- [ ] Puedo interpretar RÂ²
- [ ] SÃ© analizar residuos
- [ ] CompletÃ© el proyecto de salarios

---

**Siguiente tema**: RegresiÃ³n Polinomial y RegularizaciÃ³n
