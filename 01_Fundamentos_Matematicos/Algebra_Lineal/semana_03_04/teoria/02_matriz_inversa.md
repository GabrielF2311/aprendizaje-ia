# DÃ­a 2: Matriz Inversa

## ğŸ“‹ Objetivos del DÃ­a
- Comprender el concepto de matriz inversa
- Calcular inversas usando diferentes mÃ©todos
- Aplicar propiedades de matrices inversas
- Usar inversas para resolver sistemas lineales
- Reconocer cuÃ¡ndo NO usar inversas en Machine Learning

---

## 1. Concepto de Matriz Inversa

### 1.1 DefiniciÃ³n

La **matriz inversa** de A (denotada $A^{-1}$) es la matriz que satisface:

$$
A \cdot A^{-1} = A^{-1} \cdot A = I
$$

Donde **I** es la matriz identidad.

**Propiedades:**
- Solo matrices **cuadradas** pueden tener inversa
- No todas las matrices cuadradas tienen inversa
- Si existe, la inversa es **Ãºnica**

### 1.2 Matrices Invertibles (No Singulares)

Una matriz A es **invertible** si y solo si:
- det(A) â‰  0
- Las filas/columnas son linealmente independientes
- El rango es mÃ¡ximo (rango(A) = n para matriz nÃ—n)

**Ejemplo - Matriz 2Ã—2:**
$$
A = \begin{bmatrix} 2 & 1 \\ 3 & 4 \end{bmatrix}
$$

$$
A^{-1} = \frac{1}{5} \begin{bmatrix} 4 & -1 \\ -3 & 2 \end{bmatrix} = \begin{bmatrix} 0.8 & -0.2 \\ -0.6 & 0.4 \end{bmatrix}
$$

**VerificaciÃ³n:**
$$
A \cdot A^{-1} = \begin{bmatrix} 2 & 1 \\ 3 & 4 \end{bmatrix} \begin{bmatrix} 0.8 & -0.2 \\ -0.6 & 0.4 \end{bmatrix} = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}
$$

```python
import numpy as np

A = np.array([[2, 1],
              [3, 4]])

A_inv = np.linalg.inv(A)
print("A^(-1):")
print(A_inv)

# Verificar A @ A^(-1) = I
I = A @ A_inv
print("\nA @ A^(-1):")
print(np.round(I, 10))  # Redondear errores numÃ©ricos
# [[1. 0.]
#  [0. 1.]]
```

---

## 2. MÃ©todos de CÃ¡lculo

### 2.1 FÃ³rmula para Matriz 2Ã—2

Para matriz 2Ã—2:
$$
A = \begin{bmatrix} a & b \\ c & d \end{bmatrix}
$$

$$
A^{-1} = \frac{1}{\det(A)} \begin{bmatrix} d & -b \\ -c & a \end{bmatrix}
$$

**Pasos:**
1. Calcular det(A) = ad - bc
2. Intercambiar elementos diagonales (a â†” d)
3. Cambiar signo de elementos fuera de la diagonal
4. Dividir todo por det(A)

**Ejemplo:**
$$
A = \begin{bmatrix} 3 & 1 \\ 2 & 4 \end{bmatrix}
$$

$$
\det(A) = 3(4) - 1(2) = 10
$$

$$
A^{-1} = \frac{1}{10} \begin{bmatrix} 4 & -1 \\ -2 & 3 \end{bmatrix} = \begin{bmatrix} 0.4 & -0.1 \\ -0.2 & 0.3 \end{bmatrix}
$$

```python
def inversa_2x2(A):
    """Calcula inversa de matriz 2Ã—2"""
    a, b = A[0, 0], A[0, 1]
    c, d = A[1, 0], A[1, 1]
    
    det = a*d - b*c
    
    if abs(det) < 1e-10:
        raise ValueError("Matriz singular (det â‰ˆ 0)")
    
    return (1/det) * np.array([[d, -b],
                                [-c, a]])

A = np.array([[3, 1],
              [2, 4]], dtype=float)

A_inv = inversa_2x2(A)
print("Inversa calculada:")
print(A_inv)

# Comparar con NumPy
print("\nInversa NumPy:")
print(np.linalg.inv(A))
```

### 2.2 MÃ©todo de Gauss-Jordan

Transforma [A | I] en [I | Aâ»Â¹] usando operaciones elementales.

**Ejemplo:**
$$
A = \begin{bmatrix} 2 & 1 \\ 3 & 4 \end{bmatrix}
$$

**Paso 1:** Matriz aumentada
$$
\left[\begin{array}{cc|cc}
2 & 1 & 1 & 0 \\
3 & 4 & 0 & 1
\end{array}\right]
$$

**Paso 2:** $F_1 = F_1 / 2$
$$
\left[\begin{array}{cc|cc}
1 & 0.5 & 0.5 & 0 \\
3 & 4 & 0 & 1
\end{array}\right]
$$

**Paso 3:** $F_2 = F_2 - 3F_1$
$$
\left[\begin{array}{cc|cc}
1 & 0.5 & 0.5 & 0 \\
0 & 2.5 & -1.5 & 1
\end{array}\right]
$$

**Paso 4:** $F_2 = F_2 / 2.5$
$$
\left[\begin{array}{cc|cc}
1 & 0.5 & 0.5 & 0 \\
0 & 1 & -0.6 & 0.4
\end{array}\right]
$$

**Paso 5:** $F_1 = F_1 - 0.5F_2$
$$
\left[\begin{array}{cc|cc}
1 & 0 & 0.8 & -0.2 \\
0 & 1 & -0.6 & 0.4
\end{array}\right]
$$

$$
A^{-1} = \begin{bmatrix} 0.8 & -0.2 \\ -0.6 & 0.4 \end{bmatrix}
$$

```python
def inversa_gauss_jordan(A):
    """Calcula inversa usando Gauss-Jordan"""
    n = len(A)
    # Crear matriz aumentada [A | I]
    augmented = np.hstack([A.astype(float), np.eye(n)])
    
    # EliminaciÃ³n hacia adelante
    for i in range(n):
        # Pivoteo (opcional)
        if augmented[i, i] == 0:
            for j in range(i+1, n):
                if augmented[j, i] != 0:
                    augmented[[i, j]] = augmented[[j, i]]
                    break
        
        # Normalizar fila pivote
        augmented[i] = augmented[i] / augmented[i, i]
        
        # Eliminar debajo y arriba
        for j in range(n):
            if i != j:
                augmented[j] -= augmented[j, i] * augmented[i]
    
    # Extraer A^(-1) (lado derecho)
    return augmented[:, n:]

A = np.array([[2, 1],
              [3, 4]])

A_inv = inversa_gauss_jordan(A)
print("Inversa Gauss-Jordan:")
print(A_inv)
```

### 2.3 Usando Matriz de Cofactores (Adjunta)

Para matriz nÃ—n:

$$
A^{-1} = \frac{1}{\det(A)} \text{adj}(A)
$$

Donde **adj(A)** es la **matriz adjunta** (transpuesta de la matriz de cofactores).

**Ejemplo 3Ã—3:**
$$
A = \begin{bmatrix} 
1 & 2 & 3 \\ 
0 & 1 & 4 \\ 
5 & 6 & 0 
\end{bmatrix}
$$

**Paso 1:** Calcular cofactores
$$
C_{11} = (+1)\begin{vmatrix} 1 & 4 \\ 6 & 0 \end{vmatrix} = -24
$$

$$
C_{12} = (-1)\begin{vmatrix} 0 & 4 \\ 5 & 0 \end{vmatrix} = 20
$$

... (continuar para todos)

**Paso 2:** Matriz de cofactores â†’ Transponer â†’ Dividir por det(A)

âš ï¸ **Este mÃ©todo es ineficiente para matrices grandes** (solo Ãºtil para entender el concepto).

---

## 3. Propiedades de Matrices Inversas

### 3.1 Propiedades Algebraicas

1. **(Aâ»Â¹)â»Â¹ = A**

2. **(AB)â»Â¹ = Bâ»Â¹Aâ»Â¹** (orden invertido)

3. **(Aáµ€)â»Â¹ = (Aâ»Â¹)áµ€**

4. **det(Aâ»Â¹) = 1/det(A)**

5. **(kA)â»Â¹ = (1/k)Aâ»Â¹** (k â‰  0)

**VerificaciÃ³n:**
```python
import numpy as np

A = np.array([[2, 1], [3, 4]])
B = np.array([[1, 2], [0, 1]])

A_inv = np.linalg.inv(A)
B_inv = np.linalg.inv(B)

# Propiedad 1: (A^-1)^-1 = A
print("(A^-1)^-1 = A:")
print(np.allclose(np.linalg.inv(A_inv), A))  # True

# Propiedad 2: (AB)^-1 = B^-1 A^-1
AB_inv = np.linalg.inv(A @ B)
producto = B_inv @ A_inv
print("\n(AB)^-1 = B^-1 A^-1:")
print(np.allclose(AB_inv, producto))  # True

# Propiedad 3: (A^T)^-1 = (A^-1)^T
print("\n(A^T)^-1 = (A^-1)^T:")
print(np.allclose(np.linalg.inv(A.T), A_inv.T))  # True

# Propiedad 4: det(A^-1) = 1/det(A)
det_A = np.linalg.det(A)
det_A_inv = np.linalg.det(A_inv)
print(f"\ndet(A) = {det_A:.4f}")
print(f"det(A^-1) = {det_A_inv:.4f}")
print(f"1/det(A) = {1/det_A:.4f}")
```

### 3.2 Matrices Especiales

**Matriz Ortogonal:**
Si A es ortogonal (AAáµ€ = I), entonces:
$$
A^{-1} = A^T
$$

Calcular la inversa es trivial (solo transponer).

```python
# Ejemplo: Matriz de rotaciÃ³n (ortogonal)
theta = np.pi / 4  # 45 grados
R = np.array([[np.cos(theta), -np.sin(theta)],
              [np.sin(theta),  np.cos(theta)]])

R_inv = np.linalg.inv(R)
R_T = R.T

print("R^(-1):")
print(R_inv)
print("\nR^T:")
print(R_T)
print(f"\nÂ¿Son iguales? {np.allclose(R_inv, R_T)}")  # True
```

**Matriz Diagonal:**
Si D es diagonal:
$$
D = \begin{bmatrix} 
d_1 & 0 & 0 \\ 
0 & d_2 & 0 \\ 
0 & 0 & d_3 
\end{bmatrix}, \quad
D^{-1} = \begin{bmatrix} 
1/d_1 & 0 & 0 \\ 
0 & 1/d_2 & 0 \\ 
0 & 0 & 1/d_3 
\end{bmatrix}
$$

```python
D = np.diag([2, 3, 4])
D_inv = np.diag([1/2, 1/3, 1/4])

print("D^(-1) calculada:")
print(np.linalg.inv(D))
print("\nD^(-1) directa:")
print(D_inv)
```

---

## 4. Aplicaciones

### 4.1 ResoluciÃ³n de Sistemas Lineales

**Sistema:** Ax = b

**SoluciÃ³n:** x = Aâ»Â¹b (si A es invertible)

```python
import numpy as np

# Sistema: 2x + y = 5
#          3x + 4y = 11

A = np.array([[2, 1],
              [3, 4]])
b = np.array([5, 11])

# MÃ©todo 1: Usando inversa (âŒ menos eficiente)
A_inv = np.linalg.inv(A)
x = A_inv @ b
print(f"SoluciÃ³n (inversa): x = {x}")

# MÃ©todo 2: np.linalg.solve (âœ… mÃ¡s eficiente)
x = np.linalg.solve(A, b)
print(f"SoluciÃ³n (solve): x = {x}")

# VerificaciÃ³n
print(f"A @ x = {A @ x}")
print(f"b = {b}")
```

âš ï¸ **Importante:** En la prÃ¡ctica, **nunca** uses Aâ»Â¹ para resolver sistemas. `np.linalg.solve()` es mÃ¡s rÃ¡pido y numÃ©ricamente estable.

### 4.2 Transformaciones Inversas

```python
import numpy as np
import matplotlib.pyplot as plt

# TransformaciÃ³n: RotaciÃ³n de 45 grados
theta = np.pi / 4
R = np.array([[np.cos(theta), -np.sin(theta)],
              [np.sin(theta),  np.cos(theta)]])

# Punto original
p = np.array([1, 0])

# Aplicar rotaciÃ³n
p_rotado = R @ p

# Aplicar rotaciÃ³n inversa
R_inv = np.linalg.inv(R)
p_recuperado = R_inv @ p_rotado

print(f"Punto original: {p}")
print(f"Punto rotado: {p_rotado}")
print(f"Punto recuperado: {p_recuperado}")
print(f"Â¿Igual al original? {np.allclose(p, p_recuperado)}")
```

### 4.3 En Machine Learning

**1. RegresiÃ³n Lineal (Ecuaciones Normales):**
$$
w = (X^T X)^{-1} X^T y
$$

```python
import numpy as np
from sklearn.datasets import make_regression

# Generar datos
X, y = make_regression(n_samples=100, n_features=3, noise=10, random_state=42)

# Agregar columna de unos (bias)
X_b = np.c_[np.ones(100), X]

# Calcular pesos usando ecuaciones normales
# w = (X^T X)^(-1) X^T y
XTX = X_b.T @ X_b
XTy = X_b.T @ y
w = np.linalg.inv(XTX) @ XTy

print(f"Pesos: {w}")

# âœ… Mejor forma (sin calcular inversa explÃ­citamente)
w_mejor = np.linalg.solve(XTX, XTy)
print(f"Pesos (solve): {w_mejor}")
print(f"Â¿Iguales? {np.allclose(w, w_mejor)}")
```

âš ï¸ **Problema:** Si X tiene caracterÃ­sticas correlacionadas, Xáµ€X puede ser casi singular â†’ inversa inestable.

**2. Matriz de PrecisiÃ³n (Inversa de Covarianza):**

En estadÃ­stica multivariada:
$$
\Sigma^{-1} = \text{Matriz de PrecisiÃ³n}
$$

```python
import numpy as np

# Datos multivariados
X = np.random.randn(1000, 3)

# Matriz de covarianza
cov = np.cov(X.T)

# Matriz de precisiÃ³n (inversa de covarianza)
precision = np.linalg.inv(cov)

print("Covarianza:")
print(cov)
print("\nPrecisiÃ³n:")
print(precision)
```

**3. CalibraciÃ³n de CÃ¡maras (Computer Vision):**

Recuperar parÃ¡metros intrÃ­nsecos de la cÃ¡mara invirtiendo la matriz de proyecciÃ³n.

---

## 5. CuÃ¡ndo NO Usar Inversas

### 5.1 Matrices Grandes

**Complejidad:**
- Calcular Aâ»Â¹: O(nÂ³)
- Resolver Ax = b con Aâ»Â¹: O(nÂ³) + O(nÂ²) = O(nÂ³)
- Resolver Ax = b directamente: O(nÂ³) pero con mejores constantes

```python
import numpy as np
import time

for n in [100, 500, 1000]:
    A = np.random.rand(n, n)
    b = np.random.rand(n)
    
    # MÃ©todo 1: Inversa
    start = time.time()
    A_inv = np.linalg.inv(A)
    x1 = A_inv @ b
    tiempo_inv = time.time() - start
    
    # MÃ©todo 2: Solve
    start = time.time()
    x2 = np.linalg.solve(A, b)
    tiempo_solve = time.time() - start
    
    print(f"n={n}: Inversa={tiempo_inv:.4f}s, Solve={tiempo_solve:.4f}s")
    print(f"  Speedup: {tiempo_inv/tiempo_solve:.2f}Ã—")

# Salida ejemplo:
# n=100: Inversa=0.0015s, Solve=0.0008s - Speedup: 1.9Ã—
# n=500: Inversa=0.0420s, Solve=0.0180s - Speedup: 2.3Ã—
# n=1000: Inversa=0.1800s, Solve=0.0650s - Speedup: 2.8Ã—
```

### 5.2 Matrices Mal Condicionadas

**NÃºmero de condiciÃ³n:**
$$
\kappa(A) = \|A\| \cdot \|A^{-1}\|
$$

Si Îº(A) es grande, pequeÃ±os errores en A causan grandes errores en Aâ»Â¹.

```python
import numpy as np

# Matriz bien condicionada
A_buena = np.array([[2, 1],
                     [1, 2]])

cond_buena = np.linalg.cond(A_buena)
print(f"NÃºmero de condiciÃ³n (buena): {cond_buena:.2f}")  # ~3

# Matriz mal condicionada
A_mala = np.array([[1, 1],
                    [1, 1.0001]])

cond_mala = np.linalg.cond(A_mala)
print(f"NÃºmero de condiciÃ³n (mala): {cond_mala:.2e}")  # ~20000

# Invertir matriz mal condicionada es peligroso
A_mala_inv = np.linalg.inv(A_mala)
print("\nInversa de matriz mal condicionada:")
print(A_mala_inv)

# Verificar A @ A^-1 = I
producto = A_mala @ A_mala_inv
print("\nA @ A^(-1) (deberÃ­a ser I):")
print(producto)
# Puede tener errores numÃ©ricos significativos
```

### 5.3 Alternativas Mejores

**Para resolver Ax = b:**
- âœ… `np.linalg.solve(A, b)` - EliminaciÃ³n gaussiana
- âœ… DescomposiciÃ³n LU, QR, Cholesky (segÃºn el caso)

**Para regresiÃ³n lineal:**
- âœ… `np.linalg.lstsq(X, y)` - MÃ­nimos cuadrados (maneja matrices rectangulares)
- âœ… RegularizaciÃ³n (Ridge, Lasso) - Evita problemas de ill-conditioning

---

## 6. Pseudo-Inversa (Moore-Penrose)

Para matrices **no cuadradas** o **singulares**, existe la **pseudo-inversa** Aâº:

$$
A^+ = (A^T A)^{-1} A^T \quad \text{(si } A \text{ tiene rango completo)}
$$

**Propiedades:**
- AAâºA = A
- AâºAAâº = Aâº
- SoluciÃ³n de mÃ­nimos cuadrados: x = Aâºb

```python
import numpy as np

# Matriz no cuadrada (mÃ¡s filas que columnas)
A = np.array([[1, 2],
              [3, 4],
              [5, 6]])  # 3Ã—2

# Pseudo-inversa
A_pinv = np.linalg.pinv(A)

print(f"A shape: {A.shape}")
print(f"A^+ shape: {A_pinv.shape}")  # (2, 3)

# Verificar propiedades
print("\nA @ A^+ @ A:")
print(A @ A_pinv @ A)
print("\nÂ¿Igual a A?")
print(np.allclose(A @ A_pinv @ A, A))  # True

# Usar para resolver sistema sobredeterminado
b = np.array([1, 2, 3])
x = A_pinv @ b
print(f"\nSoluciÃ³n de mÃ­nimos cuadrados: {x}")
```

---

## 7. Errores Comunes

### âŒ Error 1: Invertir Matriz Singular
```python
A = np.array([[1, 2],
              [2, 4]])  # Filas proporcionales, det = 0

# np.linalg.inv(A)  # Â¡Error! LinAlgError: Singular matrix

# Verificar antes de invertir
if abs(np.linalg.det(A)) > 1e-10:
    A_inv = np.linalg.inv(A)
else:
    print("Matriz singular, usar pseudo-inversa")
    A_inv = np.linalg.pinv(A)
```

### âŒ Error 2: Usar Inversa en Vez de Solve
```python
A = np.random.rand(1000, 1000)
b = np.random.rand(1000)

# âŒ Ineficiente y menos preciso
x = np.linalg.inv(A) @ b

# âœ… Correcto
x = np.linalg.solve(A, b)
```

### âŒ Error 3: Asumir (A+B)â»Â¹ = Aâ»Â¹ + Bâ»Â¹
```python
A = np.array([[2, 1], [1, 2]])
B = np.array([[1, 0], [0, 1]])

# (A+B)^(-1) â‰  A^(-1) + B^(-1)
print("(A+B)^(-1):")
print(np.linalg.inv(A + B))

print("\nA^(-1) + B^(-1):")
print(np.linalg.inv(A) + np.linalg.inv(B))

# Â¡No son iguales!
```

---

## 8. Ejercicios PrÃ¡cticos

### Ejercicio 1: CÃ¡lculo Manual
Calcula la inversa de:
$$
A = \begin{bmatrix} 1 & 2 \\ 3 & 7 \end{bmatrix}
$$

### Ejercicio 2: Gauss-Jordan
Usa el mÃ©todo de Gauss-Jordan para encontrar la inversa de:
$$
A = \begin{bmatrix} 
2 & 1 & 0 \\ 
0 & 3 & 1 \\ 
1 & 0 & 2 
\end{bmatrix}
$$

### Ejercicio 3: Propiedades
Verifica que (AB)â»Â¹ = Bâ»Â¹Aâ»Â¹ para matrices 3Ã—3 aleatorias.

### Ejercicio 4: Pseudo-Inversa
Calcula la pseudo-inversa de una matriz 4Ã—2 y verifica que AAâºA = A.

---

## 9. Recursos Adicionales

### ğŸ“º Videos
- **3Blue1Brown:** "Inverse matrices, column space and null space"
- **Khan Academy:** "Invertible matrices"

### ğŸ“š Lecturas
- **Gilbert Strang:** "Introduction to Linear Algebra" - CapÃ­tulo 2

---

## ğŸ“Œ Resumen Clave

| Aspecto | Detalle |
|---------|---------|
| **DefiniciÃ³n** | AAâ»Â¹ = Aâ»Â¹A = I |
| **CondiciÃ³n** | det(A) â‰  0 |
| **2Ã—2** | FÃ³rmula directa |
| **nÃ—n** | Gauss-Jordan O(nÂ³) |
| **âš ï¸ En ML** | NO usar para sistemas, preferir `solve()` |
| **Alternativa** | Pseudo-inversa para matrices no cuadradas |

---

## ğŸ¯ PrÃ³ximos Pasos

**DÃ­a 3:** Eigenvalores y Eigenvectores
- DefiniciÃ³n y cÃ¡lculo
- InterpretaciÃ³n geomÃ©trica
- Aplicaciones en PCA

---

*La matriz inversa es fundamental en teorÃ­a, pero en la prÃ¡ctica de ML, casi nunca debes calcularla explÃ­citamente. Â¡Usa mÃ©todos numÃ©ricos mÃ¡s estables!*
