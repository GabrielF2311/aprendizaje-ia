# üìê √Ålgebra Lineal Avanzada - Semanas 3 y 4
## Eigenvalues, SVD y PCA

## üéØ Objetivos

- Calcular determinantes e inversas
- Entender eigenvalues y eigenvectors
- Aplicar descomposici√≥n SVD
- Implementar PCA desde cero
- Aplicaci√≥n real: Compresi√≥n de im√°genes

---

## üìö Contenido por D√≠a

### **D√≠a 1: Determinantes**

**Teor√≠a**:
- ¬øQu√© es un determinante?
- Interpretaci√≥n geom√©trica (√°rea/volumen)
- C√°lculo de determinantes 2x2, 3x3, nxn
- Propiedades

**Aplicaci√≥n en IA**: Determinar si una matriz es invertible

### **D√≠a 2: Matrices Inversas**

**Teor√≠a**:
- Definici√≥n de matriz inversa
- C√°lculo de inversas
- Pseudo-inversas
- M√©todo de Gauss-Jordan

**Aplicaci√≥n en IA**: Resolver sistemas lineales en regresi√≥n

### **D√≠a 3-4: Eigenvalues y Eigenvectors**

**Teor√≠a**:
- Definici√≥n: $Av = \lambda v$
- C√°lculo de eigenvalues
- C√°lculo de eigenvectors
- Diagonalizaci√≥n

**Aplicaci√≥n en IA**: 
- PCA usa eigenvectors
- An√°lisis de estabilidad en sistemas
- Spectral clustering

### **D√≠a 5: Descomposici√≥n SVD**

**Teor√≠a**:
- Singular Value Decomposition: $A = U\Sigma V^T$
- C√°lculo de SVD
- Interpretaci√≥n geom√©trica
- Reducci√≥n de rango

**Aplicaci√≥n en IA**:
- Sistemas de recomendaci√≥n
- Compresi√≥n de im√°genes
- Reducci√≥n de dimensionalidad

### **D√≠a 6: PCA (Principal Component Analysis)**

**Teor√≠a**:
- Reducci√≥n de dimensionalidad
- Encontrar componentes principales
- Varianza explicada
- Implementaci√≥n desde cero

**Aplicaci√≥n en IA**:
- Feature engineering
- Visualizaci√≥n de datos alta dimensi√≥n
- Preprocessing

### **D√≠a 7: PROYECTO - Compresi√≥n de Im√°genes**

**Objetivo**: Usar SVD para comprimir im√°genes

**Tareas**:
1. Cargar imagen como matriz
2. Aplicar SVD
3. Reconstruir con k valores singulares
4. Comparar compresi√≥n vs calidad
5. Visualizar resultados

---

## üíª Ejercicios Principales

### Ejercicio 1: Eigenvalues de una Matriz de Covarianza
```python
import numpy as np

# Matriz de covarianza de datos
cov_matrix = np.array([[4, 2], [2, 3]])

# TODO: Calcula eigenvalues y eigenvectors
# eigenvalues, eigenvectors = ...

# Interpreta: ¬øQu√© direcci√≥n tiene mayor varianza?
```

### Ejercicio 2: PCA Paso a Paso
```python
def pca_manual(X, n_components=2):
    """
    Implementa PCA desde cero.
    
    Pasos:
    1. Centrar datos (restar media)
    2. Calcular matriz de covarianza
    3. Encontrar eigenvalues/eigenvectors
    4. Ordenar por eigenvalues
    5. Proyectar datos
    """
    # TODO: Implementa esto
    pass
```

### Ejercicio 3: Compresi√≥n con SVD
```python
from PIL import Image

def compress_image(image_path, k):
    """
    Comprime imagen usando SVD.
    
    Args:
        image_path: Ruta a la imagen
        k: N√∫mero de valores singulares a mantener
    """
    # Cargar imagen
    img = np.array(Image.open(image_path).convert('L'))
    
    # TODO: Aplicar SVD
    # U, S, Vt = np.linalg.svd(img, full_matrices=False)
    
    # TODO: Reconstruir con k valores
    # img_compressed = U[:, :k] @ np.diag(S[:k]) @ Vt[:k, :]
    
    return img_compressed
```

---

## üéØ Proyecto Final: An√°lisis PCA de Dataset

**Dataset sugerido**: Iris, Wine, o MNIST simplificado

**Tareas**:
1. Cargar dataset multidimensional
2. Aplicar PCA para reducir a 2D
3. Visualizar en 2D con colores por clase
4. Analizar varianza explicada
5. Comparar clasificaci√≥n en espacio original vs PCA

**Entregables**:
- C√≥digo Python funcionando
- Visualizaciones
- An√°lisis de cu√°nta informaci√≥n se retiene
- README explicativo

---

## üîë Conceptos Clave

### Eigenvalues/Eigenvectors

Un **eigenvector** de una matriz $A$ es un vector $v$ que solo cambia de escala al multiplicarlo por $A$:

$$Av = \lambda v$$

Donde $\lambda$ es el **eigenvalue** (factor de escala).

**Intuici√≥n**: Direcciones que la transformaci√≥n solo estira/comprime.

### SVD (Descomposici√≥n en Valores Singulares)

Cualquier matriz $A_{m \times n}$ se puede descomponer en:

$$A = U\Sigma V^T$$

Donde:
- $U$: Eigenvectors de $AA^T$ (espaciorow)
- $\Sigma$: Valores singulares (ra√≠ces de eigenvalues)
- $V^T$: Eigenvectors de $A^TA$ (espacio columna)

### PCA

Encuentra las direcciones de **m√°xima varianza** en los datos.

**Algoritmo**:
1. Centrar datos: $X_{centered} = X - \text{mean}(X)$
2. Matriz de covarianza: $C = \frac{1}{n}X^TX$
3. Eigenvalues/eigenvectors de $C$
4. Proyectar: $X_{PCA} = X \cdot \text{eigenvectors}$

---

## ‚úÖ Checklist de Progreso

### Conceptos
- [ ] Entiendo qu√© es un determinante
- [ ] S√© calcular inversas de matrices
- [ ] Comprendo eigenvalues/eigenvectors
- [ ] Entiendo SVD y sus componentes
- [ ] S√© qu√© es PCA y para qu√© sirve

### Implementaci√≥n
- [ ] Calcul√© eigenvalues con NumPy
- [ ] Implement√© PCA desde cero
- [ ] Us√© SVD para compresi√≥n
- [ ] Visualic√© resultados de PCA

### Proyecto
- [ ] Comprim√≠ im√°genes con SVD
- [ ] Analic√© trade-off compresi√≥n/calidad
- [ ] Apliqu√© PCA a dataset real
- [ ] Document√© resultados

---

## üìö Recursos

### Videos
- **3Blue1Brown**: "Eigenvalues and Eigenvectors"
- **StatQuest**: "PCA Clearly Explained"
- **Computerphile**: "Singular Value Decomposition"

### Lectura
- Cap√≠tulo 7: Eigenvalues - *Linear Algebra and Its Applications*
- Cap√≠tulo 10: SVD - *Introduction to Linear Algebra*

### Interactivos
- [Visualizando Eigenvectors](http://setosa.io/ev/eigenvectors-and-eigenvalues/)
- [PCA Explicado Visualmente](http://setosa.io/ev/principal-component-analysis/)

---

## üí° Conexi√≥n con IA

### PCA en Feature Engineering
```python
from sklearn.decomposition import PCA

# Reducir 784 features (MNIST) a 50
pca = PCA(n_components=50)
X_reduced = pca.fit_transform(X_train)

# Retiene ~95% de informaci√≥n con 13x menos features!
print(f"Varianza explicada: {pca.explained_variance_ratio_.sum():.2%}")
```

### SVD en Recomendaciones
```python
# Matriz usuarios x pel√≠culas
# SVD encuentra factores latentes (g√©neros impl√≠citos)
U, S, Vt = np.linalg.svd(ratings_matrix)

# Reconstruir con k factores
k = 20
recommendations = U[:, :k] @ np.diag(S[:k]) @ Vt[:k, :]
```

---

**¬°Esta es la base matem√°tica de muchos algoritmos de ML!** üöÄ

**Siguiente**: C√°lculo y Optimizaci√≥n
