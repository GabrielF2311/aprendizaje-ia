# üéØ Operaciones con Vectores

## Producto Punto (Dot Product)

El **producto punto** es una de las operaciones m√°s importantes en √°lgebra lineal y machine learning.

### Definici√≥n

$$\vec{v_1} \cdot \vec{v_2} = v_{1,1} \cdot v_{2,1} + v_{1,2} \cdot v_{2,2} + ... + v_{1,n} \cdot v_{2,n}$$

O en forma compacta:
$$\vec{v_1} \cdot \vec{v_2} = \sum_{i=1}^{n} v_{1,i} \cdot v_{2,i}$$

### Ejemplo

```
v1 = [1, 2, 3]
v2 = [4, 5, 6]

v1 ¬∑ v2 = (1√ó4) + (2√ó5) + (3√ó6)
        = 4 + 10 + 18
        = 32
```

### Propiedades

1. **Conmutativo**: $\vec{a} \cdot \vec{b} = \vec{b} \cdot \vec{a}$
2. **Distributivo**: $\vec{a} \cdot (\vec{b} + \vec{c}) = \vec{a} \cdot \vec{b} + \vec{a} \cdot \vec{c}$
3. **Asociativo con escalar**: $(c\vec{a}) \cdot \vec{b} = c(\vec{a} \cdot \vec{b})$

### En Python

```python
# Manual
def dot_product(v1, v2):
    return sum(a * b for a, b in zip(v1, v2))

# NumPy
import numpy as np
result = np.dot(v1, v2)
# o
result = v1 @ v2  # Operador @
```

---

## Magnitud con Producto Punto

La magnitud de un vector es su producto punto consigo mismo:

$$||\vec{v}|| = \sqrt{\vec{v} \cdot \vec{v}}$$

```python
magnitude = math.sqrt(dot_product(v, v))
# O m√°s directo:
magnitude = np.linalg.norm(v)
```

---

## √Ångulo entre Vectores

### F√≥rmula

$$\cos(\theta) = \frac{\vec{v_1} \cdot \vec{v_2}}{||\vec{v_1}|| \cdot ||\vec{v_2}||}$$

Despejando Œ∏:
$$\theta = \arccos\left(\frac{\vec{v_1} \cdot \vec{v_2}}{||\vec{v_1}|| \cdot ||\vec{v_2}||}\right)$$

### Casos Especiales

| Œ∏ | cos(Œ∏) | v1 ¬∑ v2 | Interpretaci√≥n |
|---|--------|---------|----------------|
| 0¬∞ | 1 | m√°ximo positivo | Misma direcci√≥n |
| 90¬∞ | 0 | 0 | Perpendiculares |
| 180¬∞ | -1 | m√°ximo negativo | Direcciones opuestas |

### Ejemplo

```python
import math

def angle_between(v1, v2, degrees=True):
    dot = dot_product(v1, v2)
    mag1 = magnitude(v1)
    mag2 = magnitude(v2)
    
    cos_theta = dot / (mag1 * mag2)
    theta_rad = math.acos(cos_theta)
    
    if degrees:
        return math.degrees(theta_rad)
    return theta_rad

# Ejemplo
v1 = [1, 0]
v2 = [0, 1]
angle = angle_between(v1, v2)  # 90¬∞
```

---

## Vectores Perpendiculares (Ortogonales)

Dos vectores son **perpendiculares** si su producto punto es cero.

$$\vec{v_1} \perp \vec{v_2} \iff \vec{v_1} \cdot \vec{v_2} = 0$$

### Ejemplos

```python
# Perpendiculares
[1, 0] ¬∑ [0, 1] = 0  ‚úì
[3, 4] ¬∑ [-4, 3] = -12 + 12 = 0  ‚úì

# No perpendiculares
[1, 1] ¬∑ [1, 1] = 2  ‚úó
```

### Base Ortonormal

Un conjunto de vectores es **ortonormal** si:
1. Todos son perpendiculares entre s√≠ (ortogonales)
2. Todos tienen magnitud 1 (unitarios)

```python
# Base est√°ndar en 3D (ortonormal)
e1 = [1, 0, 0]
e2 = [0, 1, 0]
e3 = [0, 0, 1]

# Verifica:
e1 ¬∑ e2 = 0  # Ortogonales
||e1|| = 1   # Unitarios
```

---

## Proyecci√≥n de Vectores

La **proyecci√≥n** de $\vec{v}$ sobre $\vec{u}$ es la "sombra" de $\vec{v}$ en la direcci√≥n de $\vec{u}$.

### F√≥rmula

$$\text{proj}_{\vec{u}}(\vec{v}) = \frac{\vec{v} \cdot \vec{u}}{\vec{u} \cdot \vec{u}} \vec{u}$$

Si $\vec{u}$ es unitario (||u|| = 1):
$$\text{proj}_{\vec{u}}(\vec{v}) = (\vec{v} \cdot \vec{u}) \vec{u}$$

### Visualizaci√≥n

```
v
|
|    /
|   / proj_u(v)
|  /
| /
|/_________ u
```

### Ejemplo

```python
def project_onto(v, u):
    """Proyecta v sobre u"""
    scalar = dot_product(v, u) / dot_product(u, u)
    return [scalar * ui for ui in u]

# Proyectar [3, 4] sobre el eje X [1, 0]
v = [3, 4]
u = [1, 0]
proj = project_onto(v, u)  # [3, 0]
```

### Componentes Paralela y Perpendicular

Cualquier vector se puede descomponer en:

$$\vec{v} = \vec{v}_{\parallel} + \vec{v}_{\perp}$$

Donde:
- $\vec{v}_{\parallel}$ = proyecci√≥n sobre $\vec{u}$
- $\vec{v}_{\perp} = \vec{v} - \vec{v}_{\parallel}$

```python
v_parallel = project_onto(v, u)
v_perpendicular = [v[i] - v_parallel[i] for i in range(len(v))]
```

---

## Producto Cruz (Cross Product) - Solo 3D

El producto cruz produce un vector **perpendicular** a ambos vectores de entrada.

### F√≥rmula

$$\vec{a} \times \vec{b} = \begin{bmatrix} 
a_2b_3 - a_3b_2 \\
a_3b_1 - a_1b_3 \\
a_1b_2 - a_2b_1
\end{bmatrix}$$

### M√©todo del Determinante

$$\vec{a} \times \vec{b} = \begin{vmatrix}
\hat{i} & \hat{j} & \hat{k} \\
a_1 & a_2 & a_3 \\
b_1 & b_2 & b_3
\end{vmatrix}$$

### Ejemplo

```python
a = [1, 0, 0]  # Eje X
b = [0, 1, 0]  # Eje Y

a √ó b = [0*0 - 0*1,    # 0
         0*0 - 1*0,    # 0
         1*1 - 0*0]    # 1
      = [0, 0, 1]  # Eje Z!
```

### Propiedades

1. **No conmutativo**: $\vec{a} \times \vec{b} = -(\vec{b} \times \vec{a})$
2. **Magnitud**: $||\vec{a} \times \vec{b}|| = ||\vec{a}|| \cdot ||\vec{b}|| \cdot \sin(\theta)$
3. **Perpendicular**: $(\vec{a} \times \vec{b}) \cdot \vec{a} = 0$ y $(\vec{a} \times \vec{b}) \cdot \vec{b} = 0$

### Regla de la Mano Derecha

```
    Z (arriba)
    |
    |
    |_____ Y
   /
  /
 X

X √ó Y = Z
Y √ó Z = X
Z √ó X = Y
```

### Aplicaciones

- **F√≠sica**: Torque, momento angular
- **Geometr√≠a**: Normal a un plano (gr√°ficos 3D)
- **ML**: Menos com√∫n, pero √∫til en geometr√≠a computacional

---

## Combinaciones Lineales

Una **combinaci√≥n lineal** es:

$$c_1\vec{v_1} + c_2\vec{v_2} + ... + c_n\vec{v_n}$$

### Ejemplo

```python
# Cualquier vector en 2D se puede expresar como:
v = c1 * [1, 0] + c2 * [0, 1]

# Por ejemplo:
[3, 4] = 3 * [1, 0] + 4 * [0, 1]
```

### Espacio Generado (Span)

El **span** de un conjunto de vectores es el conjunto de todas sus combinaciones lineales posibles.

```python
# Span de [1, 0] y [0, 1] es todo R¬≤
# Cualquier punto (x, y) se puede alcanzar
```

---

## Aplicaciones en Machine Learning

### 1. Similitud de Documentos

```python
# Dos documentos representados como vectores
doc1 = [3, 1, 0, 2]  # Frecuencia de palabras
doc2 = [2, 0, 1, 1]

# Similitud = coseno del √°ngulo
similarity = dot_product(doc1, doc2) / (magnitude(doc1) * magnitude(doc2))
```

### 2. Redes Neuronales

```python
# Forward pass en una neurona
weights = [0.5, 0.3, 0.2]
inputs = [1.0, 2.0, 3.0]

# Salida = producto punto + bias
output = dot_product(weights, inputs) + bias
```

### 3. Regresi√≥n Lineal

```python
# Predicci√≥n: ≈∑ = w ¬∑ x + b
y_pred = dot_product(weights, features) + bias
```

---

## Ejercicios Conceptuales

### 1. ¬øQu√© significa un producto punto negativo?
<details>
<summary>Respuesta</summary>
El √°ngulo entre los vectores es mayor a 90¬∞ (entre 90¬∞ y 180¬∞). Los vectores "apuntan" en direcciones generalmente opuestas.
</details>

### 2. ¬øCu√°ndo es √∫til normalizar vectores antes de calcular el producto punto?
<details>
<summary>Respuesta</summary>
Cuando solo nos interesa la direcci√≥n, no la magnitud. El producto punto de vectores normalizados es exactamente cos(Œ∏), una medida directa de similitud.
</details>

### 3. ¬øPor qu√© el producto cruz solo existe en 3D?
<details>
<summary>Respuesta</summary>
En 2D no hay una direcci√≥n "perpendicular" √∫nica. En 4D+ hay m√∫ltiples direcciones perpendiculares. Solo en 3D hay exactamente una direcci√≥n perpendicular √∫nica (usando la regla de la mano derecha).
</details>

---

## Resumen de F√≥rmulas

| Operaci√≥n | F√≥rmula | Resultado |
|-----------|---------|-----------|
| **Producto Punto** | $\vec{a} \cdot \vec{b} = \sum a_ib_i$ | Escalar |
| **√Ångulo** | $\theta = \arccos\left(\frac{\vec{a} \cdot \vec{b}}{\\|\vec{a}\\| \\|\vec{b}\\|}\right)$ | √Ångulo |
| **Proyecci√≥n** | $\text{proj}_{\vec{b}}(\vec{a}) = \frac{\vec{a} \cdot \vec{b}}{\\|\vec{b}\\|^2}\vec{b}$ | Vector |
| **Producto Cruz** | $\vec{a} \times \vec{b}$ | Vector ‚ä• |

---

**Siguiente**: D√≠a 3 - Matrices y operaciones matriciales üî¢
